import os
import time
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import urlparse
import re
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By


def download_image(url, folder, index):
    """
    ä¸‹è½½å›¾ç‰‡åˆ°æŒ‡å®šæ–‡ä»¶å¤¹ã€‚
    """
    try:
        ext = os.path.splitext(urlparse(url).path)[-1].lower()
        if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
            ext = '.jpg'  # Fallback to .jpg if extension is unusual
        filename = f"image_{index}{ext}"
        filepath = os.path.join(folder, filename)

        response = requests.get(url, timeout=10, stream=True)
        response.raise_for_status()

        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        # print(f"âœ… å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {filename} from {url}") # å¯ä»¥é€‰æ‹©æ€§å¼€å¯ï¼Œé¿å…è¿‡å¤šè¾“å‡º
        return filename
    except requests.exceptions.RequestException as req_e:
        print(f"âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥ (ç½‘ç»œé—®é¢˜): {url}ï¼ŒåŸå› ï¼š{req_e}")
        return None
    except Exception as e:
        print(f"âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥ (å…¶ä»–é”™è¯¯): {url}ï¼ŒåŸå› ï¼š{e}")
        return None


def clean_filename(title):
    """
    æ¸…ç†æ ‡é¢˜ï¼Œä½¿å…¶é€‚åˆä½œä¸ºæ–‡ä»¶åã€‚
    ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œæ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿ï¼Œé™åˆ¶é•¿åº¦ã€‚
    """
    cleaned_title = re.sub(r'[^\w\s-]', '', title)
    cleaned_title = re.sub(r'\s+', '_', cleaned_title).strip()
    if len(cleaned_title) > 100:
        cleaned_title = cleaned_title[:100]
    return cleaned_title if cleaned_title else "untitled_article"  # é˜²æ­¢ç©ºæ ‡é¢˜å¯¼è‡´æ–‡ä»¶åé—®é¢˜


def fetch_wechat_article(url):
    """
    è·å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹ï¼Œå¹¶ä¿å­˜ä¸ºMarkdownæ–‡ä»¶ã€‚
    """
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # æ›´çœŸå®çš„User-Agentï¼Œæ¨¡æ‹ŸChromeæµè§ˆå™¨åœ¨Windowsç¯å¢ƒ
    chrome_options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    chrome_options.add_argument("--log-level=3")  # å‡å°‘webdriverçš„æ—¥å¿—è¾“å‡º

    service = Service(ChromeDriverManager().install())
    driver = None

    try:
        driver = webdriver.Chrome(service=service, options=chrome_options)

        print(f"ğŸš€ æ­£åœ¨åŠ è½½å¾®ä¿¡æ–‡ç« : {url}")
        driver.get(url)

        # ä½¿ç”¨WebDriverWaitç­‰å¾…é¡µé¢ä¸»è¦å…ƒç´ åŠ è½½
        try:
            # ç­‰å¾…æ–‡ç« æ ‡é¢˜æˆ–æ–‡ç« æ­£æ–‡åŒºåŸŸå‡ºç°
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.ID, "activity-name")) or
                EC.presence_of_element_located((By.CLASS_NAME, "rich_media_title")) or
                EC.presence_of_element_located((By.ID, "js_content"))
            )
            print("é¡µé¢ä¸»è¦å…ƒç´ å·²åŠ è½½ã€‚")
        except Exception as e:
            print(f"âš ï¸ ç­‰å¾…ä¸»è¦å…ƒç´ è¶…æ—¶æˆ–å¤±è´¥: {e}")
            # å³ä½¿ç­‰å¾…å¤±è´¥ä¹Ÿå°è¯•ç»§ç»­ï¼Œå¯èƒ½é¡µé¢éƒ¨åˆ†åŠ è½½äº†

        # è·å–é¡µé¢HTML
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # --- è·å–æ–‡ç« ä¸»æ ‡é¢˜ ---
        title = "æœªå‘½åæ–‡ç« "

        # ä¼˜å…ˆçº§1: é€šè¿‡ID 'activity-name'
        title_tag = soup.find("h1", {"id": "activity-name"})
        if title_tag:
            title = title_tag.get_text(strip=True)
            print(f"âœ… æˆåŠŸè·å–ä¸»æ ‡é¢˜ (é€šè¿‡ID): '{title}'")
        else:
            # ä¼˜å…ˆçº§2: é€šè¿‡class 'rich_media_title'
            title_tag = soup.find("h2", {"class": "rich_media_title"})
            if title_tag:
                title = title_tag.get_text(strip=True)
                print(f"âœ… æˆåŠŸè·å–ä¸»æ ‡é¢˜ (é€šè¿‡Class): '{title}'")
            else:
                # ä¼˜å…ˆçº§3: é€šè¿‡meta og:title (é€šå¸¸æ˜¯é¡µé¢çš„æ ‡é¢˜ï¼Œä¸æ€»æ˜¯æ–‡ç« ä¸»æ ‡é¢˜ï¼Œä½†ä½œä¸ºå¤‡ç”¨)
                meta_title_tag = soup.find("meta", property="og:title")
                if meta_title_tag and meta_title_tag.get("content"):
                    title = meta_title_tag.get("content").strip()
                    print(f"âœ… æˆåŠŸè·å–ä¸»æ ‡é¢˜ (é€šè¿‡Meta OG Title): '{title}'")
                else:
                    print("âš ï¸ æœªèƒ½é€šè¿‡å¸¸è§æ–¹å¼è·å–åˆ°æ–‡ç« ä¸»æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤æ ‡é¢˜ã€‚")

        # æ¸…ç†æ ‡é¢˜ï¼Œç”¨äºæ–‡ä»¶å
        cleaned_title = clean_filename(title)

        content_div = soup.find("div", {"id": "js_content"})
        if not content_div:
            raise Exception("âŒ æ— æ³•æ‰¾åˆ°æ–‡ç« æ­£æ–‡å†…å®¹ (#js_content)ï¼Œè¯·æ£€æŸ¥URLæˆ–é¡µé¢ç»“æ„ã€‚")

        # åˆ›å»ºç›®å½•
        folder_name = "wechat_articles"
        article_folder = os.path.join(folder_name, cleaned_title)
        img_folder = os.path.join(article_folder, "images")
        os.makedirs(img_folder, exist_ok=True)
        print(f"ğŸ“ å·²åˆ›å»ºæ–‡ç« ç›®å½•: {article_folder}")

        # æ„å»º Markdown å†…å®¹
        md_lines = [f"# {title}\n\n"]  # ä¸»æ ‡é¢˜ä½œä¸ºMarkdownçš„H1

        img_count = 0

        # éå†æ­£æ–‡å†…å®¹ï¼Œå¤„ç†æ®µè½ã€å°æ ‡é¢˜å’Œå›¾ç‰‡
        # é‡è¦çš„æ”¹è¿›ï¼šéå†js_contentçš„ç›´æ¥å­å…ƒç´ ï¼Œå¹¶é’ˆå¯¹ä¸åŒæ ‡ç­¾è¿›è¡Œå¤„ç†
        for element in content_div.children:
            if not element.name:  # å¿½ç•¥NavigableStringç±»å‹çš„æ–‡æœ¬èŠ‚ç‚¹
                continue

            # å¤„ç†å°æ ‡é¢˜ (h1-h6)
            if re.match(r'^h[1-6]$', element.name):
                # å¾®ä¿¡æ–‡ç« ä¸­çš„hæ ‡ç­¾ä¸ä¸€å®šæ˜¯çœŸçš„Hæ ‡ç­¾ï¼Œæœ‰æ—¶æ˜¯div+spanæ¨¡æ‹Ÿ
                # ä½†å¦‚æœç¡®å®æ˜¯hæ ‡ç­¾ï¼Œæˆ‘ä»¬æŒ‰å…¶çº§åˆ«è½¬æ¢ä¸ºMarkdown
                level = int(element.name[1])
                md_lines.append(f"{'#' * level} {element.get_text(strip=True)}\n")
            elif element.name == 'section' or element.name == 'p' or element.name == 'div':
                # å¤„ç†æ®µè½æ–‡æœ¬
                text = element.get_text(strip=True)
                if text:
                    md_lines.append(text + "\n")

                # å†æ¬¡å°è¯•åœ¨section/p/divå†…éƒ¨æŸ¥æ‰¾å¯èƒ½çš„â€œå°æ ‡é¢˜â€æ ·å¼
                # å¾®ä¿¡æ–‡ç« å¸¸ç”¨ style="font-size: xxx; font-weight: bold;" æˆ– span
                # è¿™æ˜¯ä¸€ä¸ªå¯å‘å¼æ–¹æ³•ï¼Œå¯èƒ½ä¸å®Œç¾
                for inner_tag in element.find_all(['strong', 'span', 'b', 'p']):
                    style = inner_tag.get('style', '')
                    # æ£€æŸ¥æ˜¯å¦æœ‰å­—ä½“å¤§å°è¾ƒå¤§ä¸”åŠ ç²—çš„æ ·å¼
                    if ("font-size" in style and "bold" in style) or \
                            ("font-size" in style and "weight:700" in style) or \
                            ("font-weight:bold" in style) or \
                            ("font-weight:700" in style):
                        inner_text = inner_tag.get_text(strip=True)
                        if inner_text and len(inner_text) > 3 and not inner_text.endswith("ã€‚"):  # é¿å…æ•è·æ™®é€šåŠ ç²—å¥å­
                            # ä¼°ç®—å°æ ‡é¢˜çº§åˆ«ï¼Œè¿™é‡Œç®€å•ç»Ÿä¸€ä¸ºäºŒçº§æ ‡é¢˜ï¼Œæˆ–æ ¹æ®å­—ä½“å¤§å°è¿›ä¸€æ­¥åˆ¤æ–­
                            # å¦‚æœæœ‰å¤šä¸ªä¸åŒå¤§å°çš„ï¼Œå¯ä»¥æ›´å¤æ‚åœ°åˆ†æ
                            md_lines.append(f"## {inner_text}\n")
                            # ç§»é™¤å·²å¤„ç†çš„å°æ ‡é¢˜æ–‡æœ¬ï¼Œé¿å…é‡å¤è¾“å‡ºä¸ºæ™®é€šæ®µè½
                            # æ³¨æ„ï¼šè¿™ä¼šä¿®æ”¹soupå¯¹è±¡ï¼Œå¯èƒ½å½±å“åç»­å¤„ç†
                            # æ›´å¥½çš„åšæ³•æ˜¯åœ¨appendä¹‹åæ ‡è®°ä¸€ä¸‹ï¼Œæˆ–è·³è¿‡
                            inner_tag.extract()  # ä»soupä¸­ç§»é™¤ï¼Œé¿å…ä½œä¸ºæ™®é€šæ–‡æœ¬å†æ¬¡æ·»åŠ 

            # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡ï¼ˆåŒ…æ‹¬åœ¨å„ç§åµŒå¥—æ ‡ç­¾å†…çš„ï¼‰
            for img in element.find_all("img"):
                img_url = img.get("data-src") or img.get("src")
                # è¿‡æ»¤æ‰ä¸€äº›å¯èƒ½æ˜¯èƒŒæ™¯å›¾æˆ–å¾ˆå°çš„å›¾æ ‡ï¼Œä¹Ÿå¯ä»¥æ£€æŸ¥ src æ˜¯å¦æ˜¯ data:image
                if img_url and img_url.startswith("http") and "wx_fmt" in img_url:  # å¾®ä¿¡å›¾ç‰‡çš„ç‰¹ç‚¹é€šå¸¸æœ‰wx_fmt
                    img_count += 1
                    local_filename = download_image(img_url, img_folder, img_count)
                    if local_filename:
                        # ç¡®ä¿ alt å±æ€§ä¸æ˜¯ None
                        alt_text = img.get('alt', '').strip()
                        if not alt_text:  # å¦‚æœaltä¸ºç©ºï¼Œå°è¯•ä»data-altè·å–
                            alt_text = img.get('data-alt', '').strip()

                        md_lines.append(f"![{alt_text}](images/{local_filename})\n")

            # å¤„ç†è§†é¢‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
            for video_tag in element.find_all("video"):
                video_src = video_tag.get("src") or video_tag.get("data-src")
                if video_src:
                    md_lines.append(f"> **[è§†é¢‘é“¾æ¥]** {video_src}\n")

            # å¤„ç†é“¾æ¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
            for a_tag in element.find_all("a"):
                link_href = a_tag.get("href")
                link_text = a_tag.get_text(strip=True)
                if link_href and link_text:
                    md_lines.append(f"[{link_text}]({link_href})\n")

        # å†™å…¥ Markdown æ–‡ä»¶
        md_filename = os.path.join(article_folder, f"{cleaned_title}.md")
        with open(md_filename, "w", encoding="utf-8") as f:
            f.write("\n".join(md_lines))

        print(f"âœ… æŠ“å–å®Œæˆï¼æ–‡ç« æ ‡é¢˜: '{title}'")
        print(f"   å…±ä¸‹è½½ {img_count} å¼ å›¾ç‰‡åˆ° '{img_folder}'")
        print(f"   Markdown æ–‡ä»¶å·²ä¿å­˜åˆ° '{md_filename}'")

    except Exception as e:
        print(f"âŒ æŠ“å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()  # æ‰“å°å®Œæ•´çš„é”™è¯¯æ ˆä¿¡æ¯ï¼Œæ–¹ä¾¿è°ƒè¯•
    finally:
        if driver:
            driver.quit()
            print("ğŸŒ Chrome æµè§ˆå™¨å·²å…³é—­ã€‚")


if __name__ == "__main__":
    # è¯·æ›¿æ¢ä¸ºä½ è¦æŠ“å–çš„å¾®ä¿¡æ–‡ç« URL
    wechat_url = "https://mp.weixin.qq.com/s/jy-tqjeV8IJAH9hTd4Oyjw"  # ä¸€ä¸ªç¤ºä¾‹æ–‡ç« 
    # wechat_url = "https://mp.weixin.qq.com/s/z926V-r1FjXk0F5v_5g2Cg" # å¦ä¸€ä¸ªç¤ºä¾‹ï¼Œå¯èƒ½æœ‰ä¸åŒçš„ç»“æ„
    # wechat_url = "https://mp.weixin.qq.com/s/YourActualArticleURLHere"
    fetch_wechat_article(wechat_url)
